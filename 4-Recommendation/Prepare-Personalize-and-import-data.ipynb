{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Personalize and import data\n",
    "\n",
    "We are going to prepare an Amazon Personalize dataset group and importing our three datasets.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "We will accomplish the following steps.\n",
    "\n",
    "- Create schema resources in Amazon Personalize that define the layout of our three dataset files (CSVs)\n",
    "- Create a dataset group in Amazon Personalize that will be used to receive our datasets\n",
    "- Create a dataset in the Personalize dataset group for the three dataset types and schemas\n",
    "    - Items: information about the products in the Retail Demo Store\n",
    "    - Users: information about the users in the Retail Deme Store\n",
    "    - Interactions: user-item interactions representing typical storefront behavior such as viewing products, adding products to a shopping cart, purchasing products, and so on\n",
    "- Create dataset import jobs to import each of the three datasets into Personalize"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Just as in the first lab, we have to prepare our environment by importing dependencies and creating clients.\n",
    "\n",
    "### Import dependencies\n",
    "\n",
    "The following libraries are needed for this lab. Install python dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install --upgrade --no-deps --force-reinstall botocore pandas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the REGION_NAME based on your environment. Setup AWS credential before execute following code.(https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-authentication.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "REGION_NAME = 'ap-northeast-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create clients\n",
    "\n",
    "We will need the following AWS service clients in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "personalize = boto3.client('personalize', region_name=REGION_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Amazon Personalize\n",
    "\n",
    "Now that we've prepared our three datasets and uploaded them to S3 we'll need to configure the Amazon Personalize service to understand our data so that it can be used to train models for generating recommendations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Schemas for Datasets\n",
    "\n",
    "Amazon Personalize requires a schema for each dataset so it can map the columns in our CSVs to fields for model training. Each schema is declared in JSON using the [Apache Avro](https://avro.apache.org/) format.\n",
    "\n",
    "Let's define and create schemas in Personalize for our datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Items Datsaset Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Items\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"PRICE\",\n",
    "            \"type\": \"float\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CATEGORY_L1\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"CATEGORY_L2\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"PRODUCT_DESCRIPTION\",\n",
    "            \"type\": \"string\",\n",
    "            \"textual\": True\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"GENDER\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"PROMOTED\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = \"retaildemostore-products-items\",\n",
    "        domain = 'ECOMMERCE',\n",
    "        schema = json.dumps(items_schema)\n",
    "    )\n",
    "    items_schema_arn = create_schema_response['schemaArn']\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You aready created this schema, seemingly')\n",
    "    paginator = personalize.get_paginator('list_schemas')\n",
    "    for paginate_result in paginator.paginate():\n",
    "        for schema in paginate_result['schemas']:\n",
    "            if schema['name'] == 'retaildemostore-products-items':\n",
    "                items_schema_arn = schema['schemaArn']\n",
    "                print(f\"Using existing schema: {items_schema_arn}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Users Dataset Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Users\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"AGE\",\n",
    "            \"type\": \"int\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"GENDER\",\n",
    "            \"type\": \"string\",\n",
    "            \"categorical\": True,\n",
    "        }\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = \"retaildemostore-products-users\",\n",
    "        domain = \"ECOMMERCE\",\n",
    "        schema = json.dumps(users_schema)\n",
    "    )\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "    users_schema_arn = create_schema_response['schemaArn']\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You aready created this schema, seemingly')\n",
    "    paginator = personalize.get_paginator('list_schemas')\n",
    "    for paginate_result in paginator.paginate():\n",
    "        for schema in paginate_result['schemas']:\n",
    "            if schema['name'] == 'retaildemostore-products-users':\n",
    "                users_schema_arn = schema['schemaArn']\n",
    "                print(f\"Using existing schema: {users_schema_arn}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactions Dataset Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_schema = {\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Interactions\",\n",
    "    \"namespace\": \"com.amazonaws.personalize.schema\",\n",
    "    \"fields\": [\n",
    "        {\n",
    "            \"name\": \"ITEM_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"USER_ID\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"EVENT_TYPE\",  # \"View\", \"Purchase\", etc.\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"TIMESTAMP\",\n",
    "            \"type\": \"long\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"DISCOUNT\",  # This is the contextual metadata - \"Yes\" or \"No\".\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "    ],\n",
    "    \"version\": \"1.0\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    create_schema_response = personalize.create_schema(\n",
    "        name = \"retaildemostore-products-interactions\",\n",
    "        domain = \"ECOMMERCE\",\n",
    "        schema = json.dumps(interactions_schema)\n",
    "    )\n",
    "    print(json.dumps(create_schema_response, indent=2))\n",
    "    interactions_schema_arn = create_schema_response['schemaArn']\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You aready created this schema, seemingly')\n",
    "    paginator = personalize.get_paginator('list_schemas')\n",
    "    for paginate_result in paginator.paginate():\n",
    "        for schema in paginate_result['schemas']:\n",
    "            if schema['name'] == 'retaildemostore-products-interactions':\n",
    "                interactions_schema_arn = schema['schemaArn']\n",
    "                print(f\"Using existing schema: {interactions_schema_arn}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Wait for Dataset Group\n",
    "\n",
    "Next we need to create the dataset group that will contain our three datasets. This is one of many Personalize operations that are asynchronous. That is, we call an API to create a resource and have to wait for it to become active."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dataset Group\n",
    "\n",
    "Note that we are also passing `ECOMMERCE` for the `domain` parameter here too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    create_dataset_group_response = personalize.create_dataset_group(\n",
    "        name = 'retaildemostore-products',\n",
    "        domain = 'ECOMMERCE'\n",
    "    )\n",
    "    dataset_group_arn = create_dataset_group_response['datasetGroupArn']\n",
    "    print(json.dumps(create_dataset_group_response, indent=2))\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You aready created this dataset group, seemingly')\n",
    "    paginator = personalize.get_paginator('list_dataset_groups')\n",
    "    for paginate_result in paginator.paginate():\n",
    "        for dataset_group in paginate_result['datasetGroups']:\n",
    "            if dataset_group['name'] == 'retaildemostore-products':\n",
    "                dataset_group_arn = dataset_group['datasetGroupArn']\n",
    "                break\n",
    "                \n",
    "print(f'DatasetGroupArn = {dataset_group_arn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Dataset Group to Have ACTIVE Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = None\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    describe_dataset_group_response = personalize.describe_dataset_group(\n",
    "        datasetGroupArn = dataset_group_arn\n",
    "    )\n",
    "    status = describe_dataset_group_response[\"datasetGroup\"][\"status\"]\n",
    "    print(\"DatasetGroup: {}\".format(status))\n",
    "    \n",
    "    if status == \"ACTIVE\" or status == \"CREATE FAILED\":\n",
    "        break\n",
    "        \n",
    "    time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Items Dataset\n",
    "\n",
    "Next we will create the datasets in Personalize for our three dataset types. Let's start with the items dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset_type = \"ITEMS\"\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = \"retaildemostore-products-items\",\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = dataset_group_arn,\n",
    "        schemaArn = items_schema_arn\n",
    "    )\n",
    "\n",
    "    items_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You aready created this dataset, seemingly')\n",
    "    paginator = personalize.get_paginator('list_datasets')\n",
    "    for paginate_result in paginator.paginate(datasetGroupArn = dataset_group_arn):\n",
    "        for dataset in paginate_result['datasets']:\n",
    "            if dataset['name'] == 'retaildemostore-products-items':\n",
    "                items_dataset_arn = dataset['datasetArn']\n",
    "                break\n",
    "                \n",
    "print(f'Items dataset ARN = {items_dataset_arn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Users Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset_type = \"USERS\"\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = \"retaildemostore-products-users\",\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = dataset_group_arn,\n",
    "        schemaArn = users_schema_arn\n",
    "    )\n",
    "\n",
    "    users_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You aready created this dataset, seemingly')\n",
    "    paginator = personalize.get_paginator('list_datasets')\n",
    "    for paginate_result in paginator.paginate(datasetGroupArn = dataset_group_arn):\n",
    "        for dataset in paginate_result['datasets']:\n",
    "            if dataset['name'] == 'retaildemostore-products-users':\n",
    "                users_dataset_arn = dataset['datasetArn']\n",
    "                break\n",
    "                \n",
    "print(f'Users dataset ARN = {users_dataset_arn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Interactions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset_type = \"INTERACTIONS\"\n",
    "    create_dataset_response = personalize.create_dataset(\n",
    "        name = \"retaildemostore-products-interactions\",\n",
    "        datasetType = dataset_type,\n",
    "        datasetGroupArn = dataset_group_arn,\n",
    "        schemaArn = interactions_schema_arn\n",
    "    )\n",
    "\n",
    "    interactions_dataset_arn = create_dataset_response['datasetArn']\n",
    "    print(json.dumps(create_dataset_response, indent=2))\n",
    "except personalize.exceptions.ResourceAlreadyExistsException:\n",
    "    print('You aready created this dataset, seemingly')\n",
    "    paginator = personalize.get_paginator('list_datasets')\n",
    "    for paginate_result in paginator.paginate(datasetGroupArn = dataset_group_arn):\n",
    "        for dataset in paginate_result['datasets']:\n",
    "            if dataset['name'] == 'retaildemostore-products-interactions':\n",
    "                interactions_dataset_arn = dataset['datasetArn']\n",
    "                break\n",
    "                \n",
    "print(f'Interactions dataset ARN = {interactions_dataset_arn}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for datasets to become active\n",
    "\n",
    "It can take a minute or two for the datasets to be created. Let's wait for all three to become active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "dataset_arns = [ items_dataset_arn, users_dataset_arn, interactions_dataset_arn ]\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    for dataset_arn in reversed(dataset_arns):\n",
    "        response = personalize.describe_dataset(\n",
    "            datasetArn = dataset_arn\n",
    "        )\n",
    "        status = response[\"dataset\"][\"status\"]\n",
    "\n",
    "        if status == \"ACTIVE\":\n",
    "            print(f'Dataset {dataset_arn} successfully completed')\n",
    "            dataset_arns.remove(dataset_arn)\n",
    "        elif status == \"CREATE FAILED\":\n",
    "            print(f'Dataset {dataset_arn} failed')\n",
    "            if response['dataset'].get('failureReason'):\n",
    "                print('   Reason: ' + response['dataset']['failureReason'])\n",
    "            dataset_arns.remove(dataset_arn)\n",
    "\n",
    "    if len(dataset_arns) > 0:\n",
    "        print('At least one dataset is still in progress')\n",
    "        time.sleep(15)\n",
    "    else:\n",
    "        print(\"All datasets have completed\")\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Datasets to Personalize\n",
    "\n",
    "In the following steps we will create import jobs with Personalize that will import the datasets from our S3 bucket into the service."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect permissions\n",
    "\n",
    "By default, the Personalize service does not have permission to acccess the data we uploaded into the S3 bucket in our account. In order to grant access to the Personalize service to read our CSVs, we need to set a Bucket Policy and create an IAM role that the Amazon Personalize service will assume.\n",
    "\n",
    "Create a bucket in S3 with following bucket policy, replace <bucket_name> with real value:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Deny\",\n",
    "            \"Principal\": \"*\",\n",
    "            \"Action\": \"s3:*\",\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::<bucket_name>\",\n",
    "                \"arn:aws:s3:::<bucket_name>/*\"\n",
    "            ],\n",
    "            \"Condition\": {\n",
    "                \"Bool\": {\n",
    "                    \"aws:SecureTransport\": \"false\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"personalize.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::<bucket_name>\",\n",
    "                \"arn:aws:s3:::<bucket_name>/*\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "And then upload users.csv, items.csv and interactions.csv to the bucket.\n",
    "\n",
    "We'll start by displaying the bucket policy in the S3 staging bucket where we uploaded the CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = '<bucket_name>'\n",
    "\n",
    "items_filename = \"items.csv\"\n",
    "users_filename = \"users.csv\"\n",
    "interactions_filename = \"interactions.csv\"\n",
    "\n",
    "s3_res = boto3.Session().resource('s3')\n",
    "s3_res.Bucket(BUCKET).Object(items_filename).upload_file(items_filename)\n",
    "s3_res.Bucket(BUCKET).Object(users_filename).upload_file(users_filename)\n",
    "s3_res.Bucket(BUCKET).Object(interactions_filename).upload_file(interactions_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\", region_name=REGION_NAME)\n",
    "\n",
    "\n",
    "response = s3.get_bucket_policy(Bucket = BUCKET)\n",
    "print(json.dumps(json.loads(response['Policy']), indent=2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create an IAM role DCEGuidancePersonalizeS3 that Personalize will need to assume to access the S3 bucket.\n",
    "Create an inline policy for this role:\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\",\n",
    "                \"s3:PutObject\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::<bucket_name>\",\n",
    "                \"arn:aws:s3:::<bucket_name>/*\"\n",
    "            ],\n",
    "            \"Effect\": \"Allow\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "We'll start by inspecting the role itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Path\": \"/\",\n",
      "  \"RoleName\": \"retaildemostore-ap-northeast-1-PersonalizeS3\",\n",
      "  \"RoleId\": \"AROATQW6HWVGVZF33GXKS\",\n",
      "  \"Arn\": \"arn:aws:iam::242057983309:role/retaildemostore-ap-northeast-1-PersonalizeS3\",\n",
      "  \"CreateDate\": \"2023-02-07 06:51:38+00:00\",\n",
      "  \"AssumeRolePolicyDocument\": {\n",
      "    \"Version\": \"2012-10-17\",\n",
      "    \"Statement\": [\n",
      "      {\n",
      "        \"Effect\": \"Allow\",\n",
      "        \"Principal\": {\n",
      "          \"Service\": \"personalize.amazonaws.com\"\n",
      "        },\n",
      "        \"Action\": \"sts:AssumeRole\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"Description\": \"\",\n",
      "  \"MaxSessionDuration\": 3600,\n",
      "  \"RoleLastUsed\": {}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "iam = boto3.client(\"iam\")\n",
    "\n",
    "role_name = \"DCEGuidancePersonalizeS3\"\n",
    "\n",
    "response = iam.get_role(RoleName = role_name)\n",
    "role_arn = response['Role']['Arn']\n",
    "print(json.dumps(response['Role'], indent=2, default = str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the role has the same service principal as the bucket policy but this time with the `sts:AssumeRole` action. This is required so that Personalize can assume this role.\n",
    "\n",
    "Finally, we'll get the inline policy named `BucketAccess` that has the same S3 permissions as the bucket policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"RoleName\": \"retaildemostore-ap-northeast-1-PersonalizeS3\",\n",
      "  \"PolicyName\": \"BucketAccess\",\n",
      "  \"PolicyDocument\": {\n",
      "    \"Version\": \"2012-10-17\",\n",
      "    \"Statement\": [\n",
      "      {\n",
      "        \"Action\": [\n",
      "          \"s3:GetObject\",\n",
      "          \"s3:ListBucket\"\n",
      "        ],\n",
      "        \"Resource\": [\n",
      "          \"arn:aws:s3:::retail-demo-store-ap-northeast-1\",\n",
      "          \"arn:aws:s3:::retail-demo-store-ap-northeast-1/*\",\n",
      "          \"arn:aws:s3:::retaildemostore-base-w4qexcd5hhdg-buc-stackbucket-dsli02y8esva\",\n",
      "          \"arn:aws:s3:::retaildemostore-base-w4qexcd5hhdg-buc-stackbucket-dsli02y8esva/*\"\n",
      "        ],\n",
      "        \"Effect\": \"Allow\"\n",
      "      },\n",
      "      {\n",
      "        \"Action\": [\n",
      "          \"s3:PutObject\"\n",
      "        ],\n",
      "        \"Resource\": [\n",
      "          \"arn:aws:s3:::retaildemostore-base-w4qexcd5hhdg-buc-stackbucket-dsli02y8esva\",\n",
      "          \"arn:aws:s3:::retaildemostore-base-w4qexcd5hhdg-buc-stackbucket-dsli02y8esva/*\"\n",
      "        ],\n",
      "        \"Effect\": \"Allow\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"c5e08d25-090c-4455-b810-d231721a65d9\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"x-amzn-requestid\": \"c5e08d25-090c-4455-b810-d231721a65d9\",\n",
      "      \"content-type\": \"text/xml\",\n",
      "      \"content-length\": \"1217\",\n",
      "      \"date\": \"Fri, 07 Jul 2023 06:26:27 GMT\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = iam.get_role_policy(RoleName = role_name, PolicyName = 'BucketAccess')\n",
    "print(json.dumps(response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Import Jobs\n",
    "\n",
    "With the permissions in place to allow Personalize to access our CSV files, let's create three import jobs to import each file into its respective dataset. Each import job can take several minutes to complete so we'll create all three import jobs and then wait for them all to complete. This allows them to import in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Items Dataset Import Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:ap-northeast-1:242057983309:dataset-import-job/retaildemostore-products-items-f31e19b2\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"fb9d20c9-af39-4944-988e-246800739425\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 07 Jul 2023 07:10:16 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"132\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"fb9d20c9-af39-4944-988e-246800739425\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import_job_suffix = str(uuid.uuid4())[:8]\n",
    "\n",
    "items_create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"retaildemostore-products-items-\" + import_job_suffix,\n",
    "    datasetArn = items_dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket, items_filename)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "items_dataset_import_job_arn = items_create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(items_create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Users Dataset Import Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:ap-northeast-1:242057983309:dataset-import-job/retaildemostore-products-users-f31e19b2\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"04c8ca35-aed8-4226-af49-30ba3f0d1158\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 07 Jul 2023 07:10:19 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"132\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"04c8ca35-aed8-4226-af49-30ba3f0d1158\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "users_create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"retaildemostore-products-users-\" + import_job_suffix,\n",
    "    datasetArn = users_dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket, users_filename)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "users_dataset_import_job_arn = users_create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(users_create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Interactions Dataset Import Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"datasetImportJobArn\": \"arn:aws:personalize:ap-northeast-1:242057983309:dataset-import-job/retaildemostore-products-interactions-f31e19b2\",\n",
      "  \"ResponseMetadata\": {\n",
      "    \"RequestId\": \"f7d6ec00-5d2a-425c-9a7c-db54253cce74\",\n",
      "    \"HTTPStatusCode\": 200,\n",
      "    \"HTTPHeaders\": {\n",
      "      \"date\": \"Fri, 07 Jul 2023 07:10:22 GMT\",\n",
      "      \"content-type\": \"application/x-amz-json-1.1\",\n",
      "      \"content-length\": \"139\",\n",
      "      \"connection\": \"keep-alive\",\n",
      "      \"x-amzn-requestid\": \"f7d6ec00-5d2a-425c-9a7c-db54253cce74\"\n",
      "    },\n",
      "    \"RetryAttempts\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "interactions_create_dataset_import_job_response = personalize.create_dataset_import_job(\n",
    "    jobName = \"retaildemostore-products-interactions-\" + import_job_suffix,\n",
    "    datasetArn = interactions_dataset_arn,\n",
    "    dataSource = {\n",
    "        \"dataLocation\": \"s3://{}/{}\".format(bucket, interactions_filename)\n",
    "    },\n",
    "    roleArn = role_arn\n",
    ")\n",
    "\n",
    "interactions_dataset_import_job_arn = interactions_create_dataset_import_job_response['datasetImportJobArn']\n",
    "print(json.dumps(interactions_create_dataset_import_job_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait for Import Jobs to Complete\n",
    "\n",
    "It will take 10-15 minutes for the import jobs to complete, while you're waiting you can learn more about Datasets and Schemas here: https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html\n",
    "\n",
    "We will wait for all three jobs to finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait for Items Import Job to Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At least one dataset import job still in progress\n",
      "At least one dataset import job still in progress\n",
      "At least one dataset import job still in progress\n",
      "At least one dataset import job still in progress\n",
      "Import job arn:aws:personalize:ap-northeast-1:242057983309:dataset-import-job/retaildemostore-products-interactions-f31e19b2 successfully completed\n",
      "At least one dataset import job still in progress\n",
      "Import job arn:aws:personalize:ap-northeast-1:242057983309:dataset-import-job/retaildemostore-products-users-f31e19b2 successfully completed\n",
      "Import job arn:aws:personalize:ap-northeast-1:242057983309:dataset-import-job/retaildemostore-products-items-f31e19b2 successfully completed\n",
      "All import jobs have ended\n",
      "CPU times: user 206 ms, sys: 67.2 ms, total: 274 ms\n",
      "Wall time: 5min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import_job_arns = [ items_dataset_import_job_arn, users_dataset_import_job_arn, interactions_dataset_import_job_arn ]\n",
    "\n",
    "max_time = time.time() + 3*60*60 # 3 hours\n",
    "while time.time() < max_time:\n",
    "    for job_arn in reversed(import_job_arns):\n",
    "        import_job_response = personalize.describe_dataset_import_job(\n",
    "            datasetImportJobArn = job_arn\n",
    "        )\n",
    "        status = import_job_response[\"datasetImportJob\"]['status']\n",
    "\n",
    "        if status == \"ACTIVE\":\n",
    "            print(f'Import job {job_arn} successfully completed')\n",
    "            import_job_arns.remove(job_arn)\n",
    "        elif status == \"CREATE FAILED\":\n",
    "            print(f'Import job {job_arn} failed')\n",
    "            if import_job_response[\"datasetImportJob\"].get('failureReason'):\n",
    "                print('   Reason: ' + import_job_response[\"datasetImportJob\"]['failureReason'])\n",
    "            import_job_arns.remove(job_arn)\n",
    "\n",
    "    if len(import_job_arns) > 0:\n",
    "        print('At least one dataset import job still in progress')\n",
    "        time.sleep(60)\n",
    "    else:\n",
    "        print(\"All import jobs have ended\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 2 Summary - What have we accomplished?\n",
    "\n",
    "In this lab we created schemas in Amazon Personalize that mapped to the dataset CSVs we created in Lab 1. We also created a dataset group in Personalize as well as datasets to receive our CSVs. Since Personalize needs access to the staging S3 bucket where the CSVs were uploaded, we inspected the S3 bucket policy and IAM role that needs to be passed to Personalize. Finally, we create dataset import jobs in Personalize to upload the three datasets into Personalize.\n",
    "\n",
    "In the next lab we will create the recommenders and custom solutions and solution versions for our personalization use cases. This is where the machine learning models are trained and deployed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store variables needed in the next lab\n",
    "\n",
    "We will pass some variables initialized in this lab by storing them in the notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'dataset_group_arn' (str)\n",
      "Stored 'role_arn' (str)\n"
     ]
    }
   ],
   "source": [
    "%store dataset_group_arn\n",
    "%store role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue to [Lab 3](./Lab-3-Create-recommenders-and-custom-solutions.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
